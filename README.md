# Solution
## Installation and Usage
I used Python 3.11.9 and Poetry for this project. I generally use Pyenv to manage my Python versions. I also included 
a requirements.txt generated by Poetry that should also be usable with pip (e.g. `pip install -r requirements.txt`), if
you aren't experienced with poetry for python dependency management. I did use type annotations, since it's my habit,
so Python 3.11 is necessary.

You can install Poetry by running:
```bash
curl -sSL https://install.python-poetry.org | python3 -
```
Alternatively, you can use pipx if you already have it installed (this is the method I generally use since it keeps
things clean and isolated from the rest of my system):
```bash
pipx install poetry
```
After installing poetry, you can install the dependencies using `poetry install`. You can then run the script by 
entering the poetry shell with `poetry shell` and then running the script with `python main.py <path-to-index-file>`. 
The index file path supports both a URL and a local file path (but the URL will take about the same time as downloading
the file would, so be prepared for it to take a while). It's useful if you're limited on disk space though.

## Code Explanation
The code is in `main.py`. It creates a stream of the index file, either a local stream or a stream from a URL, and then 
uses python's builtin gzip library to decompress the file. It then uses `ijson` to stream specific JSON objects at paths
within the file (https://pypi.org/project/ijson/), extracting individual `reporting_structure` objects and processing 
each of them one at a time. If the reporting structure object has a plan in `reporting_plans` that matches one of the 
plans I identified in my EDA (in `notebooks/Explore Reporting Plans.ipynb`), it then extracts the in-network file URLs
from the `in_network` field and builds up a set of URLs. It then writes the set to a file, `SOLUTION.txt`, as well as 
storing a file of URLs + descriptions as a parquet in data/reporting_plans.parquet.

## Development Process
I started out by trying to stream the file, since I previously have streamed zip files remotely for ETL processes in 
order to save on disk space, but I quickly realized that that was going to lead to a *very* long cycle between repeated
tests of my script so it would only really be useful in production. I then downloaded the file and began to explore it.

I then extracted the different reporting plans stored in the reporting_structure[].reporting_plans[] path, and stored 
them in a parquet file so that I could explore them in a notebook quickly. I did this in 
`notebooks/Explore Reporting Plans.ipynb`. Using that notebook, I found a set of EINs that seemed to correspond to plans
that were corresponding to businesses associated with the Anthem NY PPO. I also tried (abortively) to parse the plan 
names in a rigorous way but I realized that was distracting me from completing the task at hand, since it was more 
complex than I initially believed.

I tried restricting the in network file URLs to only those that had the EINs I found, but I realized that EINs could 
be associated with several plans, since, despite them appearing as the 'plan_id' here, they are actually employer IDs, 
and employers can have multiple plans. I then decided to use the 'plan_name' field to filter the plans that I had
identified as belonging to the Anthem NY PPO. I used this for my solution.

I tried to check my work using the code in check_work.py and an EIN I found in my EDA (45-2320063) but the URLs that I
extracted from the HTML and the URLs I extracted from the index file didn't match, but I was out of time at this point,
so I'm not sure precisely what the bug is, though I know it has something to do with not restricting to the right plans,
since it looks like some stuff from different states than NY is showing up there.

It took me about 2 hours to write the code, and on my machine it would run on a local file in ~2 minutes. When I ran it
on a remote file, TQDM estimated the time required as somewhere between 1 hour and 30 minutes and 2 hours and 30 
minutes, but I only waited for it to finish that way once. I wrote it in two sessions, first for just ~30 minutes when I
was trying the streaming technique, and then for the rest of the time the next night, once I'd actually downloaded the 
file. I then spent about 30 minutes cleaning up the code and cleaning up this README from my notes as I'd been working.

## Tradeoffs
Parsing the plan names is something I would have liked to have done cleanly, but I didn't have time to finish my 
approach. Likewise, I would have wanted to go back and verify my work in greater detail, but didn't have time. Coming 
back to:
- When you look at your output URL list, which segments of the URL are changing, which segments are repeating, and what might that mean?
- Is the 'description' field helpful? Is it complete? Does it change relative to 'location'? Is Highmark the same as Anthem?

It's clear without much analysis that the URLs are distributed across several different systems (e.g. somewhere in S3, 
but also at dedicated URLs like https://empirebcbs.mrf.bcbs.com and there is a date-based organizational structure for
a lot of them, but I didn't get a chance to look into this with as much detail as I would have liked. I also didn't get 
a chance to compare the description fields to the location fields, but they did not seem particularly helpful in my
cursory examination.
